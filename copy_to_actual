#!/bin/bash

# Define the base directories
TEMP_DIR="/user/username/db/cdh6"
ACTUAL_DIR="/user/username/db"

# Path to the file that contains table names
TABLES_FILE="/path/to/tables.txt"

# Define Beeline connection details (adjust as needed)
BEELINE_URL="jdbc:hive2://localhost:10000/default"
BEELINE_USER="your_username"
BEELINE_PASS="your_password"

# Read each line (table name) from the file
while IFS= read -r table_name; do
    echo "Processing table: $table_name"

    # Get the list of partition directories under the temp location for the current table
    partitions=$(hadoop fs -ls "$TEMP_DIR/$table_name" | awk '{print $8}')

    for partition in $partitions; do
        # Define the source and destination paths
        source_path="$partition"
        destination_path="$ACTUAL_DIR/$table_name"

        # Copy partition directory from temp location to actual location
        echo "Copying from $source_path to $destination_path"
        hadoop fs -cp "$source_path" "$destination_path"
    done

    # After copying all partitions, run MSCK REPAIR TABLE using Beeline
    echo "Running MSCK REPAIR TABLE on $table_name"
    beeline -u "$BEELINE_URL" -n "$BEELINE_USER" -p "$BEELINE_PASS" --silent=true --outputformat=tsv2 -e "MSCK REPAIR TABLE $table_name"

done < "$TABLES_FILE"

echo "Data copy and table repair completed."
